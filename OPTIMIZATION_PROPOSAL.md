# Предложение по снижению задержки ответа бота

## 1. Наблюдаемое узкое место
- По итоговой документации задержка при создании новых embeddings составляет 1.27–1.30 секунды и признана главным узким местом текущей системы.【F:OPTIMIZATION_TASK_CLAUDE.md†L395-L414】
- В рабочих логах видно, что каждый новый запрос инициирует блокирующий вызов OpenAI embeddings, который занимает 0.65–1.16 секунды, прежде чем начнётся генерация ответа.【F:Логи_работы_бота.txt†L66-L69】【F:Логи_работы_бота.txt†L280-L284】【F:Логи_работы_бота.txt†L379-L382】【F:Логи_работы_бота.txt†L498-L501】
- Пока embedding создаётся, параллельно воспроизводится филлер, но фактическая задержка до первого осмысленного аудио остаётся ~3 секунды, поскольку LLM не может стартовать без вектора запроса.【F:OPTIMIZATION_TASK_CLAUDE.md†L312-L414】

## 2. Предлагаемая оптимизация: локальный слой embeddings + ленивый fallback
Цель — убрать сетевую задержку на этапе генерации embeddings для новых вопросов и тем самым сократить путь ASR → первый чанк ответа минимум на 0.8–1.1 секунды.

### 2.1 Архитектура
1. **Локальная модель embeddings**
   - Добавить зависимость `sentence-transformers` и использовать модель уровня `intfloat/multilingual-e5-small` или `paraphrase-multilingual-MiniLM-L12-v2`. Эти модели дают вектор за ~80–120 мс на CPU.
   - При развертывании один раз пересчитать документы базы знаний этой локальной моделью и сохранить в отдельной коллекции Chroma (например, `kb_local`).

2. **Двухуровневый ретривер**
   - В `Agent` создать сервис `LocalRetriever`, который сначала пытается найти документы через локальные embeddings (быстро, без API вызова).
   - Если уверенность/косинусное сходство выше порога (например, 0.78), отправляем эти документы в LLM и сразу стартуем chunked streaming.
   - Параллельно (через `asyncio.create_task`) запускаем существующий вызов OpenAI embeddings как fallback. Если результат локального ретривера оказался слабее порога или fallback вернёт заметно лучшие документы, можно обновить кеш и использовать их для следующих запросов.

3. **Интеграция в обработчик звонка**
   - В `stasis_handler_optimized.py` после получения `normalized_text` стартовать две параллельные задачи: воспроизведение филлера (как сейчас) и локальный ретривер.
   - Как только локальный ретривер вернул документы, немедленно запускать `parallel_tts` с подготовленным контекстом. Время ожидания ограничится временем локального инференса (~0.1 с) + сетевой латентностью LLM (~1.5–2.3 с).

### 2.2 Ожидаемый выигрыш
- **Embeddings**: снижение с 0.65–1.16 с до ~0.1 с (CPU). Экономия ≈0.55–1.05 с на каждый новый вопрос.
- **Общая задержка**: для сценария «новый вопрос» получаем прогноз 2.2–2.5 с вместо 3.5–4.0 с, что укладывается в целевой коридор 2.2–2.7 с, заявленный в документации как достижимый при параллелизации embeddings.【F:OPTIMIZATION_TASK_CLAUDE.md†L400-L414】
- **Стабильность**: отсутствие сетевых колебаний OpenAI на критическом пути уменьшит разброс ответа (с текущих 0.38–1.16 с на embeddings по логам до стабильных 0.1–0.15 с).

## 3. План внедрения
1. **Подготовка окружения**
   - Добавить `sentence-transformers` в `requirements.txt` и снабдить проект скриптом миграции, который прогоняет базу знаний через локальную модель и сохраняет `kb_local`.
2. **Рефакторинг агента**
   - Вынести создание embeddings в отдельные методы: `get_query_embedding()` и `retrieve_with_vector()`.
   - Добавить класс `HybridRetriever`, который сначала обращается к локальной коллекции, а затем при необходимости запускает OpenAI fallback (в фоне, обновляя Redis-кеш).
   - Обеспечить совместимость с текущим кешем: ключ кеша формируется из нормализованного текста + типа модели, чтобы различать локальные и облачные векторы.
3. **Изменения в обработчике звонка**
   - После ASR запускать `asyncio.create_task(agent.hybrid_prepare_response(...))`, которая возвращает уже найденный контекст и потоковой генератор.
   - Исключить лишний `await asyncio.sleep(0.20)` перед запуском генерации; достаточно дождаться подтверждения, что филлер стартовал (можно опционально слушать `ParallelTTSProcessor.performance_metrics`).
4. **Мониторинг**
   - Добавить новые метрики: `local_embedding_latency`, `fallback_embedding_latency`, `hybrid_hit_ratio`.
   - В логах фиксировать, какой слой (локальный или облачный) обслужил запрос, чтобы оценивать качество и частоту fallback.

## 4. Риски и пути смягчения
- **Точность локальной модели**: русскоязычные технические запросы могут требовать экспериментов с несколькими моделями. Решение — добавить A/B логику: если LLM ответил с низкой уверенностью или пользователь переспрашивает, помечать запрос как требующий облачного embeddings по умолчанию.
- **Ресурсы сервера**: sentence-transformers на CPU требует 200–300 МБ RAM. Нужно проверить наличие ресурсов на прод-сервере.
- **Миграция БЗ**: единоразовое пересоздание коллекции может занять несколько минут; стоит выполнить заранее и держать старую коллекцию до завершения тестов.

## 5. Следующие шаги
1. Подготовить прототип `LocalRetriever` и протестировать оффлайн на 50–100 реальных запросах (метрики: latency, точность ответов).
2. Внедрить гибридный режим на staging и сравнить среднюю задержку «ASR → FIRST AUDIO» по логам.
3. После подтверждения качества включить по умолчанию и держать fallback на OpenAI для редких случаев.

Ожидаемый результат — выравнивание задержки для новых вопросов с кешированными кейсами (≈2.0–2.3 секунды) без ухудшения качества ответов.
