# Предложение оптимизации: асинхронное прогревание embeddings

## Краткое резюме

По логам видно, что даже после внедренных оптимизаций создание нового embedding для пользовательского запроса занимает от **0.39 до 1.16 секунды**, что напрямую добавляет задержку ко времени появления первого аудио чанка при каждом "холодном" вопросе.【F:Логи_работы_бота.txt†L66-L109】【F:Логи_работы_бота.txt†L270-L319】【F:Логи_работы_бота.txt†L360-L420】【F:Логи_работы_бота.txt†L488-L537】 Документ с выполненной работой прямо отмечает генерацию embeddings как главное узкое место, которое мешает опуститься ниже 3 секунд при новых вопросах.【F:OPTIMIZATION_TASK_CLAUDE.md†L17-L47】 При этом текущая реализация начинает получать ответ от агента только после паузы для проигрывания филлера.【F:asterisk-vox-bot/app/backend/asterisk/stasis_handler_optimized.py†L373-L398】 В итоге время на сетевой вызов OpenAI Embeddings тратится последовательно и никак не скрывается.

## Идея

Параллелизовать фазу получения embedding запроса с уже существующей задержкой на проигрывание мгновенного филлера. Как только ASR вернул нормализованный текст, мы:

1. Проверяем Redis‑кеш (существующее поведение `CachedOpenAIEmbeddings`).
2. Если embedding отсутствует, **запускаем его создание в фоне** (через `asyncio.create_task` + `loop.run_in_executor`), пока пользователь слышит филлер.
3. Когда `Agent.get_response_generator()` стартует поток RAG, embedding уже лежит в Redis, поэтому LangChain мгновенно достаёт его из кеша.

Так мы перекрываем 0.4–1.2 секунды сетевой задержки OpenAI и приближаемся к обещанным 2.2–2.7 секундам "time-to-first-audio" даже для новых запросов.

## Предлагаемый план внедрения

1. **Новый метод агента `prefetch_query_embedding`:**
   - Принимает текст и ключ БЗ.
   - Быстро проверяет Redis на наличие embedding; если ключ найден — мгновенно завершает работу.
   - Если embedding отсутствует, вызывает существующий `CachedOpenAIEmbeddings.embed_query()` внутри `asyncio.get_running_loop().run_in_executor(None, ...)`, чтобы не блокировать event loop.
   - Хранит `asyncio.Task` в локальном словаре, чтобы повторно не запускать ту же работу при одинаковом вопросе.

2. **Запуск префетча в обработчике речи:**
   - Сразу после нормализации текста и до `await asyncio.sleep(0.20)` создаём задачу `self.agent.ensure_prefetched_embedding(normalized_text, target_kb)`.
   - Филлер продолжает проигрываться без изменений, а сетевой вызов на embedding идёт в фоне.

3. **Повторное использование результата:**
   - LangChain внутри `get_response_generator()` (и всех цепочек `Chroma` → `OpenAIEmbeddings`) автоматически обратится к Redis. Поскольку embedding уже прогрет, ответ придёт из кеша без ожидания сети.
   - Для надёжности можно добавить явный `await self.agent.wait_for_prefetch(normalized_text)` перед запуском цепочки, если требуется 100% гарантия готовности.

4. **Логирование и метрики:**
   - Логируем время, за которое embedding оказался готов. Это позволит убедиться, что вторая стадия действительно проходит мгновенно.
   - Добавляем счётчик попаданий в кеш после прогрева, чтобы отслеживать эффективность.

## Ожидаемый эффект

- Скрываем **0.4–1.2 с** сетевой задержки на новые embeddings (по логам).
- При текущем среднем TTFT 0.77–3.0 с получаем общую задержку **≈2.2–2.7 с** даже для "холодных" запросов, что соответствует целевому уровню, описанному в документации.【F:OPTIMIZATION_TASK_CLAUDE.md†L40-L47】
- Нагрузка на OpenAI API не меняется, так как количество вызовов сохраняется.

## Возможные риски и способы их снижения

- **Дублирование запросов при одинаковых вопросах одновременно:** используем словарь активных задач, чтобы повторно использовать уже запущенный префетч.
- **Сбой сети во время фоновой задачи:** оборачиваем `embed_query` в повторную попытку/логирование ошибок; при падении префетч не мешает основному потоку — цепочка всё равно повторно вызовет `embed_query` (как сейчас).
- **Потребление ресурсов:** фоновые задачи короткие и редкие, поэтому дополнительных ограничений не требуется. При большом количестве параллельных звонков можно ограничить размер пула префетчей.

## Дальнейшие шаги

1. Реализовать описанные методы (`Agent.prefetch_query_embedding`, `Agent.wait_for_prefetch`).
2. Интегрировать вызов в `_process_user_text` до паузы на 200 мс.
3. Провести A/B измерение: сравнить метрику "FIRST AUDIO PLAYED" для новых вопросов до и после изменения.

Такой подход минимально инвазивен, использует уже существующую инфраструктуру кеширования и позволяет снять главное оставшееся узкое место без изменения моделей или инфраструктуры.
